{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple twitter sentiment analysis. This data science project includes the following steps - \n",
    "\n",
    "1. Generation of twitter data\n",
    "2. Cleaning the tweets\n",
    "3. Determining the sentiments of the tweets and interpreting the sentiment of the tweets on basis of positive and negative words\n",
    "4. Stop words are omitted in the analysis\n",
    "5. Calculation of overall sentiment on the topic based on the sentiments deduced from the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated using an easy to use twitter library called the Tweepy. This library allows users to gain access to twitter data. The first step in obtaining twitter data is creating application on https://apps.twitter.com/. \n",
    "\n",
    "This allows users to obtain consumer_key, consumer_secret, access_token, access_token_secret. This data is stored in the file config.py. It will enable a user to extract data from Twitter using the Tweepy library.\n",
    "\n",
    "Tweepy currently allows upto a maximum of 200 tweets to be extracted in one go. However, using custom function, multiple tweets are extracted.\n",
    "\n",
    "Users can change the number of tweets and the data to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import codecs\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up twitter\n",
    "\n",
    "def twitter_setup():\n",
    "    auth = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an object for extraction of data\n",
    "tweet_extractor = twitter_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the topic you want to find sentiment of: trump\n",
      "Enter the number of Tweets you want to download: 200\n"
     ]
    }
   ],
   "source": [
    "# Extraction of tweets\n",
    "all_tweets = []\n",
    "\n",
    "search_term=input(\"Enter the topic you want to find sentiment of: \")\n",
    "max_tweets=int(input(\"Enter the number of Tweets you want to download: \"))\n",
    "\n",
    "#max limit to extract tweet is 200 per request\n",
    "new_tweets = tweet_extractor.user_timeline(screen_name = search_term , count=200)\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "\n",
    "oldest = all_tweets[-1].id - 1\n",
    "   \n",
    "#keep extracting tweets till there are no tweets left\n",
    "\n",
    "while len(new_tweets) < max_tweets:\n",
    "    \n",
    "    #max_id has been used to prevent duplicates\n",
    "    new_tweets = tweet_extractor.user_timeline(screen_name = search_term,count=200,max_id=oldest)\n",
    "\n",
    "    #most recent tweets are saved in the all_tweets list\n",
    "    all_tweets.extend(new_tweets)\n",
    "        \n",
    "    #id of the oldest tweet is updated -1\n",
    "    oldest = all_tweets[-1].id - 1\n",
    "\n",
    "    print(\"%s tweets downloaded so far\" % (len(all_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets have been written in the file Tweets.txt\n"
     ]
    }
   ],
   "source": [
    "#Writing the extracted tweets in the Tweets.txt file\n",
    "file = codecs.open(\"Tweets.txt\", \"w\", \"utf-8\")\n",
    "for tweet in all_tweets:\n",
    "    file.write(tweet.text)\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "file.close()\n",
    "\n",
    "\n",
    "print(\"\\nTweets have been written in the file Tweets.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#For running multiple outputs in Jupyter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the tweets using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process of cleaning the tweet using regular expressions\n",
    "#Following changes are done to clean the tweet - \n",
    "\n",
    "#Conversion to lower case\n",
    "#Conversion of URLs in form of www.* or http://* or https://* to URL\n",
    "#Conversion of @username to text AT_USER\n",
    "#Removing all special characters\n",
    "#Replacing all hashtag words to simple words\n",
    "#Stripping additional space\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweets = tweet.lower()    \n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+)|(https:/[^\\s]+)|(http?[^\\s]+))','URL',tweet)    \n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)    \n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet=re.sub(r'[^\\w]', ' ', tweet)\n",
    "    \n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    tweet_list=tweet.split()\n",
    "    \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of stop words for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "#Adding more parts like usernames, URL to the list of stop words\n",
    "\n",
    "def stopword_list(stop_words):\n",
    "    stop_words.append('AT_USER')\n",
    "    stop_words.append('URL')\n",
    "    stop_words.append('RT')\n",
    "    stop_words.append('a')\n",
    "    return stop_words\n",
    "\n",
    "stop_words= stopword_list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the positive and negative word list files to identify these words\n",
    "\n",
    "positive_tweets=open('positive-words.txt','r').read().split(\"\\n\")\n",
    "negative_tweets=open('negative-words.txt','r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign sentiment value to each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(final_tweet):\n",
    "    count=0\n",
    "    for words in final_tweet:\n",
    "        if words in positive_tweets:\n",
    "            count+=1\n",
    "        if words in negative_tweets:\n",
    "            count-=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### Determine sentiment from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is positive\n",
      "Sentiment score = 133\n"
     ]
    }
   ],
   "source": [
    "final_tweet=[]\n",
    "processed_tweet=[]\n",
    "\n",
    "#Read the file line by line \n",
    "fp = open('Tweets.txt', 'r',encoding=\"utf8\")\n",
    "line = fp.readline()\n",
    "while line:\n",
    "    line = fp.readline()\n",
    "    \n",
    "    #Add the cleaned tweets to list clean tweets\n",
    "    clean_tweets=clean_tweet(line)\n",
    "    \n",
    "    #Stopwords are removed from the clean_tweets list\n",
    "    \n",
    "    for i in clean_tweets:\n",
    "        if i in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            processed_tweet.append(i) \n",
    "            \n",
    "final_tweet.extend(processed_tweet) \n",
    "\n",
    "#Add sentiment to words for each tweet\n",
    "count=find_sentiment(final_tweet)\n",
    "if count > 0:\n",
    "    print('The sentiment is positive')\n",
    "elif count <0:\n",
    "    print('The sentiment is negative')\n",
    "else:\n",
    "    print('The sentiment is neutral')\n",
    "\n",
    "print(\"Sentiment score = %d\" % count)\n",
    "\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Total Positive/Negative word count which was found in previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive word count = 143\n",
      "Negative word count = 10\n"
     ]
    }
   ],
   "source": [
    "positive_count=0\n",
    "negative_count=0\n",
    "for word in final_tweet:\n",
    "    if word not in stop_words:\n",
    "        if word in positive_tweets:\n",
    "            positive_count+=1\n",
    "        elif word in negative_tweets:\n",
    "            negative_count+=1\n",
    "            \n",
    "print(\"Positive word count = %d\" % positive_count)\n",
    "print(\"Negative word count = %d\" % negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like the sentiment of users on this topic is Positive.\n"
     ]
    }
   ],
   "source": [
    "if positive_count>negative_count:\n",
    "    print (\"It looks like the sentiment of users on this topic is Positive.\")\n",
    "else:\n",
    "    print (\"It looks like the sentiment of users on this topic is Negative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting tweets more than 200- https://gist.github.com/yanofsky/5436496 <br>\n",
    "\n",
    "For positive and negative word list- http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html <br>\n",
    "\n",
    "For cleaning the tweets- https://www.ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/#implementation-details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
